defaults:
  - _self_

type: lqh
embedding_layer_sizes: [256, 256]
embedding_activation: relu
dk: 64
num_latents: 16
latent_num_heads: 2
latent_head_features: 16
ff_mult: 2
attn_dropout: 0.0
ff_dropout: 0.0
use_self_attention: false
